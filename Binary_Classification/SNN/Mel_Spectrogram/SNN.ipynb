{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky neuron model, overriding the backward pass with a custom function\n",
    "class LeakySigmoidSurrogate(nn.Module):\n",
    "  def __init__(self, beta, threshold=1.0, k=25):\n",
    "\n",
    "      # Leaky_Surrogate is defined in the previous tutorial and not used here\n",
    "      super(Leaky_Surrogate, self).__init__() \n",
    "\n",
    "      # initialize decay rate beta and threshold\n",
    "      self.beta = beta\n",
    "      self.threshold = threshold\n",
    "      self.surrogate_func = self.FastSigmoid.apply\n",
    "  \n",
    "  # the forward function is called each time we call Leaky\n",
    "  def forward(self, input_, mem):\n",
    "    spk = self.surrogate_func((mem-self.threshold))  # call the Heaviside function\n",
    "    reset = (spk - self.threshold).detach()\n",
    "    mem = self.beta * mem + input_ - reset\n",
    "    return spk, mem\n",
    "\n",
    "  # Forward pass: Heaviside function\n",
    "  # Backward pass: Override Dirac Delta with gradient of fast sigmoid\n",
    "  @staticmethod\n",
    "  class FastSigmoid(torch.autograd.Function):  \n",
    "    @staticmethod\n",
    "    def forward(ctx, mem, k=25):\n",
    "        ctx.save_for_backward(mem) # store the membrane potential for use in the backward pass\n",
    "        ctx.k = k\n",
    "        out = (mem > 0).float() # Heaviside on the forward pass: Eq(1)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): \n",
    "        (mem,) = ctx.saved_tensors  # retrieve membrane potential\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input / (ctx.k * torch.abs(mem) + 1.0) ** 2  # gradient of fast sigmoid on backward pass: Eq(4)\n",
    "        return grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "batch_size = 1\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "def split_train(data_folder, train_fraction,transform):\n",
    "    # print(train_data_folder)\n",
    "    data_train = ImageFolder(data_folder, transform)\n",
    "    train, test = random_split(data_train, [int(train_fraction*len(data_train)),len(data_train)-int(train_fraction*len(data_train))], generator=torch.Generator().manual_seed(42))\n",
    "    return train, test\n",
    "\n",
    "def prepare_data_classes_from_train(src_data_folder, dest_data_folder, train_labels_csv_file, move = False):\n",
    "    data = pd.read_csv(train_labels_csv_file)\n",
    "    labels_dic = {}\n",
    "    for id, pos_label in zip(data['id'], data['pos_label']):\n",
    "        labels_dic[str(id).split(\".\")[0]] = int(pos_label)\n",
    "    # print(data.groupby('pos_label').count())\n",
    "    n_classes = data.groupby('pos_label').count().count()[0]\n",
    "\n",
    "    if os.path.exists(f'{dest_data_folder}'):\n",
    "        shutil.rmtree(f'{dest_data_folder}')\n",
    "    os.mkdir(f'{dest_data_folder}')\n",
    "\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        if os.path.exists(f'{dest_data_folder}/{cls}'):\n",
    "            shutil.rmtree(f'{dest_data_folder}/{cls}')\n",
    "        os.mkdir(f'{dest_data_folder}/{cls}')\n",
    "\n",
    "    for file in os.listdir(src_data_folder):\n",
    "        # print(labels_dic[file.split('.')[0]])\n",
    "        # print(f'{dest_data_folder}/{str(labels_dic[file.split(\".\")[0]])}/{file}')\n",
    "\n",
    "        try:            \n",
    "            # print(f'{src_data_folder}/{file} ==> {dest_data_folder}/{str(labels_dic[file.split(\".\")[0]])}/{file}')\n",
    "            if move:\n",
    "                shutil.move(f'{src_data_folder}/{file}', f'{dest_data_folder}/{str(labels_dic[file.split(\".\")[0]])}/{file}')\n",
    "            else:\n",
    "                shutil.copy(f'{src_data_folder}/{file}', f'{dest_data_folder}/{str(labels_dic[file.split(\".\")[0]])}/{file}')\n",
    "        except Exception:\n",
    "            print(f'Error in copying ... file: {file}')\n",
    "\n",
    "    return n_classes\n",
    "\n",
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "    \n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total\n",
    "\n",
    "def forward_pass(net, num_steps, data):\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps):\n",
    "      spk_out, mem_out = net(data)\n",
    "      spk_rec.append(spk_out)\n",
    "      mem_rec.append(mem_out)\n",
    "  \n",
    "  return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "data_folder = 'snn_original1'\n",
    "dest_data_folder = 'snn_original_classes'\n",
    "\n",
    "prepare_data_classes_from_train(data_folder, dest_data_folder, 'labels.csv', move=False)\n",
    "X_train, X_test = split_train(dest_data_folder, 0.8, transform)\n",
    "train_dataloader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(X_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = split_train(dest_data_folder, 0.8, transform)\n",
    "train_dataloader = DataLoader(X_train, shuffle=True)\n",
    "test_dataloader = DataLoader(X_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1684264355777,
     "user": {
      "displayName": "Farah Farah",
      "userId": "02593084481793888222"
     },
     "user_tz": -120
    },
    "id": "AoYBY89angvp"
   },
   "outputs": [],
   "source": [
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, threshold=0.6, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Conv2d(12, 64, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, threshold=0.4, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(64*53*53, 10),\n",
    "                    snn.Leaky(beta=beta, threshold=0.2, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_dataloader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    spk_out, mem_out = net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data):\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps):\n",
    "      spk_out, mem_out = net(data)\n",
    "      spk_rec.append(spk_out)\n",
    "      mem_rec.append(mem_out)\n",
    "  \n",
    "  return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = forward_pass(net, num_steps, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SF.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "    \n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "num_epochs = 20\n",
    "loss_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training loop\n",
    "    for data, targets in iter(train_dataloader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "        \n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        test_acc = batch_accuracy(test_dataloader, net, num_steps)\n",
    "        print(f'iteration {counter}, Test Acc: {test_acc * 100:.2f}%\\n')\n",
    "        test_acc_hist.append(test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(train_acc_hist)\n",
    "plt.plot(test_acc_hist)\n",
    "plt.legend([\"Train Accuracy\", \"Test Accuracy\"])\n",
    "plt.title(\"Accuracy Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('accuracy_curves.svg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
